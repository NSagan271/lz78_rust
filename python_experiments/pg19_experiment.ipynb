{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 12:49:45.389599: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-23 12:49:45.405198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737665385.423887 2019935 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737665385.429580 2019935 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-23 12:49:45.448989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/nsagan/micromamba/envs/lz_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import Sequence, CharacterMap, BlockLZ78Encoder, LZ78SPA\n",
    "from lz78 import encoded_sequence_from_bytes, spa_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG19DataLoader:\n",
    "    def __init__(self, data_type: str, start_index: int = 0, batch_size: int = 1, normalize: str = 'none'):\n",
    "        self.data = tfds.load('pg19', split=data_type, shuffle_files=False)\n",
    "        self.dataset = (self.data\n",
    "                        .skip(start_index)\n",
    "                        .batch(batch_size)\n",
    "                        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "        print(data_type, \": \", len(self.dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            text_bytes = np.frombuffer(batch['book_text'].numpy()[0], dtype=np.uint8)\n",
    "            text_bytes = text_bytes.tolist()\n",
    "            yield text_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  28602\n",
      "validation :  50\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PG19DataLoader(\"train\")\n",
    "val_dataloader = PG19DataLoader(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at /data/home/nsagan/LZ78-implementation/lz78_rust/crates/lz78/src/spa/lz_transform.rs:266:17:\n",
      "hi 100_000_000\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "hi 100_000_000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mspa_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../spa_outputs/pg19_spa.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPanicException\u001b[0m: hi 100_000_000"
     ]
    }
   ],
   "source": [
    "stdout.flush()\n",
    "model = spa_from_file(\"../spa_outputs/pg19_spa.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.00390625,\n",
       " 'adaptive_gamma': 'Disabled',\n",
       " 'n_ensemble': 10,\n",
       " 'ensemble_type': 'Average',\n",
       " 'backshift_parsing': True,\n",
       " 'backshift_ctx_len': 6,\n",
       " 'backshift_min_count': 1,\n",
       " 'lb': 0.0001,\n",
       " 'temp': 1.0,\n",
       " 'lb_or_temp_first': 'lb_first'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_inference_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_inference_params(ensemble_type='depth', ensemble_n=6, backshift_ctx_len=0, lb=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "n_bytes = 0\n",
    "for (i, inf_batch) in tqdm(enumerate(val_dataloader), desc=\"Validation\"):\n",
    "    stdout.flush()\n",
    "    new_loss = model.compute_test_loss(Sequence(inf_batch, alphabet_size=256))\n",
    "    print(new_loss / len(inf_batch))\n",
    "    loss += new_loss\n",
    "    n_bytes += len(inf_batch)\n",
    "    model.reset_state()\n",
    "\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "print(f\"total: {loss / n_bytes}, ppl: {2**(loss / n_bytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.5,\n",
       " 'adaptive_gamma': 'Disabled',\n",
       " 'n_ensemble': 1,\n",
       " 'ensemble_type': 'Disabled',\n",
       " 'backshift_parsing': True,\n",
       " 'backshift_ctx_len': 5,\n",
       " 'backshift_min_count': 1,\n",
       " 'lb': 0.0001,\n",
       " 'temp': 1.0,\n",
       " 'lb_or_temp_first': 'lb_first'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_inference_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_inference_params(ensemble_type=\"average\", ensemble_n=10, backshift_ctx_len=2, backshift_min_count=0, lb=1e-5, temp=1, gamma=1/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  12%|█▏        | 6/50 [00:52<06:16,  8.55s/it]"
     ]
    }
   ],
   "source": [
    "for inf_batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "    stdout.flush()\n",
    "    model.compute_test_loss(Sequence(inf_batch, alphabet_size=256))\n",
    "    model.reset_state()\n",
    "    # val_byte += len(inf_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building LZ tree:   0%|          | 0/28602 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bytes:  125104 ; Training log loss:  4.554748789335367\n",
      "Running inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/50 [00:16<?, ?it/s]\n",
      "Building LZ tree:   0%|          | 0/28602 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inf_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(val_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     stdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m---> 18\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_test_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minf_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphabet_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mreset_state()\n\u001b[1;32m     20\u001b[0m     val_byte \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inf_batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stdout.flush()\n",
    "total_byte = 0\n",
    "for trn_iter, batch in enumerate(tqdm(train_dataloader, desc=\"Building LZ tree\"), start=1):\n",
    "    # build LZ model only 1 epoch\n",
    "    stdout.flush()\n",
    "    model.train_on_block(Sequence(batch, alphabet_size=256))\n",
    "    model.reset_state()\n",
    "    total_byte += len(batch)\n",
    "\n",
    "    if trn_iter % 10 == 1:\n",
    "        print(\"Total bytes: \", total_byte, \"; Training log loss: \", model.get_normalized_log_loss())\n",
    "        print(\"Running inference\")\n",
    "        val_loss = 0\n",
    "        val_byte = 0\n",
    "\n",
    "        for inf_batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            stdout.flush()\n",
    "            val_loss += model.compute_test_loss(Sequence(inf_batch, alphabet_size=256))\n",
    "            model.reset_state()\n",
    "            val_byte += len(inf_batch)\n",
    "        print(\"Inference log loss: \", val_loss / val_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
