{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LZ78 Embeddings: Simple Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from tqdm import tqdm\n",
    "from lz_embed.transformer_based import LZPlusEmbeddingModel, WeightType, EmbeddingType, \\\n",
    "    TokenizedLZPlusEmbedding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import ndcg_score\n",
    "import mteb\n",
    "\n",
    "from model2vec import StaticModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import regex as re\n",
    "from model2vec.distill.inference import create_output_embeddings_from_model_and_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TokenizedLZPlusEmbedding`\n",
    "This does the following:\n",
    "1. Computes embeddings for all tokens in (e.g.) the BERT vocabulary and performs PCA, just like the Potion models. \n",
    "\n",
    "    _Note_: this part is currently suboptimal compared to the Potion models, which do an additional round of finetuning for the stored embeddings. We can very well do this, but it would make the initial experiment cycle quite unwieldy.\n",
    "\n",
    "2. Trains an LZ78 SPA on some data (making everything lowercase and omitting everything that isn't a letter, number, or space)\n",
    "\n",
    "3. Computes embeddings by taking the weighted average of the stored embeddings over the tokens in the input sequences.\n",
    "Here, we have the option to use uniform weighting, Zipf weighting (which should be equivalent to the Potion models, minus the finetuning), or LZ log loss weighing.\n",
    "\n",
    "Theoretically, the LZ log loss weighting should be better, but that's currently not the case. Probably the LZ tree needs to be trained on more data. It's better than uniform weighting, which is at least a step in the right direction.\n",
    "\n",
    "**Some future steps**:\n",
    "- Train LZ on more data\n",
    "- Try replacing the BERT tokenizer with an LZ-based tokenizer\n",
    "- Train LZ on tokens instead of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TokenizedLZPlusEmbedding( \n",
    "    inner_model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    output_dir=\"object\",\n",
    "    compute_device=\"cuda:7\",\n",
    "    weight_type=WeightType.LOG_LOSS,\n",
    "    pca_dim=256,\n",
    "    pca=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"salesforce/Wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "stdout.flush()\n",
    "for _ in tqdm(range(EPOCHS)):\n",
    "    model.train_spa([text[\"text\"] for text in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.spa.prune(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The LZ tree has {model.spa.get_total_nodes() / 1e6} million nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTEB Evaluation\n",
    "As the `LZPlusEmbeddingModel` class inherits from `SentenceTransformer`, any MTEB task can be evaluated using the `mteb` library's interface.\n",
    "\n",
    "Below are a few from the benchmark, with a very high-level description of how the task is scored.\n",
    "\n",
    "### AILA Statutes (Retrieval)\n",
    "We are given some documents and queries. The embedding model is scored based on whether the relevant documents for each query are close to the query in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spa.set_inference_config(\n",
    "    lb=1e-3,\n",
    "    gamma=1/model.charmap.alphabet_size(),\n",
    "    ensemble_type=\"entropy\",\n",
    "    ensemble_n=20,\n",
    "    backshift_ctx_len=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.LOG_LOSS\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"AILAStatutes\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.ZIPF\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"AILAStatutes\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.UNIFORM\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"AILAStatutes\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXivHierarchicalClusteringP2P (Clustering)\n",
    "We are given articles from Arxiv, and the embedding model is scored based on how well embeddings of the articles can be hierarchically clustered (compared to ground-truth \"topic\" labels for the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.LOG_LOSS\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"ArXivHierarchicalClusteringP2P\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.ZIPF\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"ArXivHierarchicalClusteringP2P\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.UNIFORM\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"ArXivHierarchicalClusteringP2P\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpediaClassification\n",
    "Some classification task for encyclopedia articles, scored based on accuracy. Classification appears to be performed based on k-nearest-neighbors in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.LOG_LOSS\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"DBpediaClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.ZIPF\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"DBpediaClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.UNIFORM\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"DBpediaClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTopicSingleClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.LOG_LOSS\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"TweetTopicSingleClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test_2021\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.ZIPF\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"TweetTopicSingleClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test_2021\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.UNIFORM\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"TweetTopicSingleClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test_2021\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoemSentimentClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.LOG_LOSS\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"PoemSentimentClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.ZIPF\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"PoemSentimentClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight_type = WeightType.UNIFORM\n",
    "\n",
    "tasks = mteb.get_tasks(tasks=[\"PoemSentimentClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")\n",
    "\n",
    "print(\"SCORE: \", results[0].scores[\"test\"][0][\"main_score\"] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
