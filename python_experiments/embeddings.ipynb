{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LZ78 Embeddings: Simple Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from tqdm import tqdm\n",
    "from lz_embed.transformer_based import LZPlusEmbeddingModel, WeightType, EmbeddingType\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.metrics import ndcg_score\n",
    "import mteb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Model Training\n",
    "This is super non-optimal! Just training on Wikipedia and computing embeddings on the fly (as opposed to caching + PCA, which is perhaps the ultimate goal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LZPlusEmbeddingModel(\n",
    "    # inner_model_name=\"text-embedding-3-large\",\n",
    "    inner_model_name=\"Alibaba-NLP/gte-Qwen2-7B-instruct\",\n",
    "    device=\"cuda:7\",\n",
    "    inner_model_type=EmbeddingType.TRANSFORMERS,\n",
    "    valid_character_string=\"abcdefghijklmnopqrstuvwxyz \",\n",
    "    make_lowercase=True,\n",
    "    weight_type=WeightType.UNIFORM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"salesforce/Wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "stdout.flush()\n",
    "for _ in tqdm(range(EPOCHS)):\n",
    "    for text in (dataset[\"train\"]):\n",
    "        text = text[\"text\"]\n",
    "        if not text:\n",
    "            continue\n",
    "        model.train_spa(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spa.prune(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The LZ tree has {model.spa.get_total_nodes() / 1e6} million nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_subspace(\n",
    "    512, num_gen_seqs=2000, gen_seq_len=250, backshift_len=6,\n",
    "    enable_low_rank_projection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTEB Evaluation\n",
    "As the `LZPlusEmbeddingModel` class inherits from `SentenceTransformer`, any MTEB task can be evaluated using the `mteb` library's interface.\n",
    "\n",
    "Below are a few from the benchmark, with a very high-level description of how the task is scored.\n",
    "\n",
    "### AILA Statutes (Retrieval)\n",
    "We are given some documents and queries. The embedding model is scored based on whether the relevant documents for each query are close to the query in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(tasks=[\"AILAStatutes\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].scores[\"test\"][0][\"main_score\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXivHierarchicalClusteringP2P (Clustering)\n",
    "We are given articles from Arxiv, and the embedding model is scored based on how well embeddings of the articles can be hierarchically clustered (compared to ground-truth \"topic\" labels for the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(tasks=[\"ArXivHierarchicalClusteringP2P\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].scores[\"test\"][0][\"v_measure\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpediaClassification\n",
    "Some classification task for encyclopedia articles, scored based on accuracy. Classification appears to be performed based on k-nearest-neighbors in embedding space.\n",
    "\n",
    "**Warning**: this task takes substatially longer than the the previous two; about 45 minutes on an A6000 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(tasks=[\"DBpediaClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "# If this doesn't actually run, you'll have to delete a JSON file in results/test\n",
    "results = evaluation.run(\n",
    "    model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].scores[\"test\"][0][\"accuracy\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes: Next Steps\n",
    "- (implemented) _lowercase and ignore punctuation_\n",
    "- can take the embeddings as we go instead of just at leaves\n",
    "- SoTA instead of Qwen; can use an API\n",
    "    - Tried this; OpenAI embeddings are quite slow\n",
    "- Focus on wikipedia classification to see if we should train on more data (in-distribution)\n",
    "- (implemented) _Monte Carlo -> PCA to get subspace_\n",
    "- Ablate on different averaging methods\n",
    "- Try to do a zipf weighting\n",
    "    - This might be morally similar to doing a plain average of the weights; TODO---think more about this point\n",
    "\n",
    "### Plan\n",
    "- First thing we should ablate is the weights\n",
    "- Our method has a unique advatage of giving an accurate perplexity estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
