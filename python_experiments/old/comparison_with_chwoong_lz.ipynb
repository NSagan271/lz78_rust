{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import Sequence, LZ78SPA\n",
    "# from lz_python.lz import LZModel\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from sys import stdout\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG19DataLoader:\n",
    "    def __init__(self, data_type: str, start_index: int = 0, batch_size: int = 1, normalize: str = 'none'):\n",
    "        self.data = tfds.load('pg19', split=data_type, shuffle_files=False)\n",
    "        self.dataset = (self.data\n",
    "                        .skip(start_index)\n",
    "                        .batch(batch_size)\n",
    "                        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "        print(data_type, \": \", len(self.dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            text_bytes = np.frombuffer(batch['book_text'].numpy()[0], dtype=np.uint8)\n",
    "            text_bytes = text_bytes.tolist()\n",
    "            yield text_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigObject:\n",
    "    def __init__(self, config_dict):\n",
    "        self.__dict__.update(config_dict)\n",
    "\n",
    "config = ConfigObject({\n",
    "    \"top_k\": 256,\n",
    "    \"method\": \"Depth-Guided\", # ensemble\n",
    "    \"ensemble_max_num\": 6,\n",
    "    \"min_depth\": 10,\n",
    "    \"vocab_size\": 256,\n",
    "    \"adaptive_gamma\": \"none\",\n",
    "    \"gamma\": 1/256,\n",
    "    \"lower_bound\": 1e-5,\n",
    "    \"temp\": 1,\n",
    "    \"ensemble_type\": \"depth\",\n",
    "    \"lb_or_temp\": \"lb_first\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_lz = LZModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rust_lz = LZ78SPA(alphabet_size=256, gamma=1/256, compute_training_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 100\n",
    "\n",
    "stdout.flush()\n",
    "train_dataloader = PG19DataLoader(\"train\")\n",
    "rust_lz.reset_state()\n",
    "for trn_iter, batch in enumerate(tqdm(train_dataloader, desc=\"Building LZ tree\"), start=1):\n",
    "    # build LZ model only 1 epoch\n",
    "    stdout.flush()\n",
    "    rust_lz.train_on_block(Sequence(batch, alphabet_size=256))\n",
    "    rust_lz.reset_state()\n",
    "\n",
    "    if trn_iter >= N_TRAIN:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = PG19DataLoader(\"train\")\n",
    "for trn_iter, batch in enumerate(tqdm(train_dataloader, desc=\"Building LZ tree\"), start=1):\n",
    "    # build LZ model only 1 epoch\n",
    "    py_lz.build_tree(batch)\n",
    "\n",
    "    if trn_iter >= N_TRAIN:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rust_lz.set_inference_config(\n",
    "    lb=1e-5,\n",
    "    temp=1,\n",
    "    lb_or_temp_first=\"lb_first\",\n",
    "    ensemble_type=\"depth\",\n",
    "    ensemble_n=6,\n",
    "    adaptive_gamma=\"disabled\",\n",
    "    backshift_parsing=True,\n",
    "    backshift_ctx_len=10,\n",
    "    backshift_break_at_phrase=True\n",
    ")\n",
    "\n",
    "py_lz.config = ConfigObject({\n",
    "    \"top_k\": 256,\n",
    "    \"method\": \"Depth-Guided\", # ensemble\n",
    "    \"ensemble_max_num\": 6,\n",
    "    \"min_depth\": 10,\n",
    "    \"vocab_size\": 256,\n",
    "    \"adaptive_gamma\": \"none\",\n",
    "    \"gamma\": 1/256,\n",
    "    \"lower_bound\": 1e-5,\n",
    "    \"temp\": 1,\n",
    "    \"ensemble_type\": \"depth\",\n",
    "    \"lb_or_temp\": \"lb_first\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = PG19DataLoader(\"validation\")\n",
    "test_seq = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = []\n",
    "for i in range(0, len(test_seq)-1023, 512):\n",
    "    test_seqs.append(test_seq[i:i+1024])\n",
    "\n",
    "test_seqs = test_seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rust_lz.compute_test_loss_parallel(\n",
    "    [Sequence(seq, alphabet_size=256) for seq in test_seqs], output_patch_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "\n",
    "inputs = [Sequence(seq[512:],alphabet_size=256) for seq in test_seqs]\n",
    "ctxs = [Sequence(seq[:512],alphabet_size=256) for seq in test_seqs]\n",
    "\n",
    "res = rust_lz.compute_test_loss_parallel(\n",
    "    inputs, ctxs, num_threads=32, output_prob_dists=False, output_per_symbol_losses=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([x[\"avg_log_loss\"] for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_lz_losses = []\n",
    "for seq in test_seqs:\n",
    "    depths, btb, _ = py_lz.get_depth_and_perplexity(seq)\n",
    "    py_lz_losses.append(float(np.mean(btb)))\n",
    "print(np.array(py_lz_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Full Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss = 0\n",
    "n_seqs = 0\n",
    "val_dataloader = PG19DataLoader(\"validation\")\n",
    "for seq in tqdm(val_dataloader):\n",
    "    stdout.flush()\n",
    "    test_seqs = []\n",
    "    for i in range(0, len(seq)-1023, 512):\n",
    "        test_seqs.append(seq[i:i+1024])\n",
    "\n",
    "    rust_lz.reset_state()\n",
    "\n",
    "    inputs = [Sequence(seq[64:],alphabet_size=256) for seq in test_seqs]\n",
    "    ctxs = [Sequence(seq[:64],alphabet_size=256) for seq in test_seqs]\n",
    "\n",
    "    res = rust_lz.compute_test_loss_parallel(\n",
    "        inputs, ctxs, num_threads=32, output_prob_dists=False, output_per_symbol_losses=False\n",
    "    )\n",
    "\n",
    "    log_loss += np.sum(np.array([x[\"avg_log_loss\"] for x in res]))\n",
    "    n_seqs += len(test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Val loss: {float(log_loss / n_seqs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Patch Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = PG19DataLoader(\"validation\")\n",
    "test_seq = next(iter(val_dataloader))[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rust_lz.compute_test_loss( # also works for the parallel version!\n",
    "    Sequence(test_seq, alphabet_size=256), output_prob_dists=False, output_per_symbol_losses=True, output_patch_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks reasonable; should maybe debug more\n",
    "for info in res['patch_info']:\n",
    "    print(f\"{info[0]} through {info[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.stem(np.array(res['log_losses']))\n",
    "plt.title(\"Log Loss per Symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
