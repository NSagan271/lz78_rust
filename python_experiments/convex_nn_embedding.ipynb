{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsagan/micromamba/envs/mb/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import regex as re\n",
    "from model2vec.distill.tokenizer import remove_tokens\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cvx\n",
    "from model2vec import StaticModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example: IMDB Classification\n",
    "\n",
    "### Baselines\n",
    "**Model2Vec**: 65%\n",
    "\n",
    "**Potion**: 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"mteb/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_name: str, potion_model_name: str, device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name, trust_remote_code=True, device=device)\n",
    "        full_vocab = [pair[0] for pair in sorted(self.tokenizer.get_vocab().items(), key=lambda x: x[1])]\n",
    "        vocab = [x for x in full_vocab if not re.match(\"\\[unused\\d+\\]\", x)]\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.tokenizer = remove_tokens(self.tokenizer.backend_tokenizer, set(full_vocab) - set(vocab))\n",
    "        self.tokenizer.no_padding()\n",
    "\n",
    "        model = StaticModel.from_pretrained(potion_model_name)\n",
    "        self.projection = model.embedding\n",
    "\n",
    "    def get_token_counts(self, texts: list[str] | str, progress=False):\n",
    "        if type(texts) == str:\n",
    "            texts = [texts]\n",
    "        encoded = self.tokenizer.encode_batch([x.lower() for x in texts], add_special_tokens=False)\n",
    "        token_counts = np.zeros((len(texts), self.vocab_size))\n",
    "        \n",
    "        iterator = enumerate(encoded)\n",
    "        if progress:\n",
    "            iterator = enumerate(tqdm(encoded))\n",
    "        for (row, enc) in iterator:\n",
    "            for id in enc.ids:\n",
    "                token_counts[row, id] += 1\n",
    "        return token_counts\n",
    "    \n",
    "    def get_token_count_projection(self, texts: list[str] | str, progress=False):\n",
    "        counts = self.get_token_counts(texts, progress=progress)\n",
    "        return counts @ self.projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrapper(\"BAAI/bge-base-en-v1.5\", \"minishlab/potion-base-2M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:04<00:00, 6060.57it/s]\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.get_token_count_projection(data[\"train\"][\"text\"], progress=True)\n",
    "y = np.array(data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:09<00:00, 2723.84it/s]\n"
     ]
    }
   ],
   "source": [
    "Xtest = tokenizer.get_token_count_projection(data[\"test\"][\"text\"], progress=True)\n",
    "ytest = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is mostly copied from Mert's Mosek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    return x>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Mosek parameter dictionary from Mert\n",
    "# params = {\n",
    "#       \"MSK_IPAR_NUM_THREADS\": 8,\n",
    "#       #\"MSK_IPAR_INTPNT_MAX_ITERATIONS\": 10,\n",
    "#       #\"MSK_IPAR_OPTIMIZER\": 0 # auto 0, interior point 1, conic 2\n",
    "#       #\"MSK_DPAR_INTPNT_CO_TOL_REL_GAP\": 1e-2\n",
    "#       #\"MSK_DPAR_INTPNT_TOL_PSAFE\": 0.01\n",
    "#       #\"MSK_IPAR_OPTIMIZER\": \"free\"\n",
    "#       #\"MSK_IPAR_INTPNT_SOLVE_FORM\": 1\n",
    "#       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(\n",
    "    X: np.array, y: np.array, \n",
    "    Xtest: np.array, ytest: np.array,\n",
    "    seed=0, hidden_dim=2000,\n",
    "    weight_decay_strength=None,\n",
    "    verbose=True,\n",
    "    mosek_params={\"MSK_IPAR_NUM_THREADS\": 64}\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    n,d = X.shape\n",
    "    ntest = Xtest.shape[0]\n",
    "\n",
    "    if weight_decay_strength is None:\n",
    "        weight_decay_strength = np.array([0, 1, 5e-1, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    # Say the two-layer neural network is relu(X @ U1) @ U2.\n",
    "    # Then, the convex version of the neural network requires\n",
    "    # knowing all possible formations of indic{X @ U1 > 0}, where\n",
    "    # the indicator function is taken element-wise. This is\n",
    "    # computationally expensive, so we estimate it by randomly\n",
    "    # sampling the matrix U1.\n",
    "    U1 = np.random.randn(d,hidden_dim)\n",
    "    dmat = drelu(X @ U1)\n",
    "    \n",
    "    dmat, ind=(np.unique(dmat,axis=1, return_index=True))\n",
    "    m1=dmat.shape[1]\n",
    "    U=U1[:,ind]\n",
    "\n",
    "    # CVXPY variables for finite-dimensional optimization problem\n",
    "    # from Section 3 of https://arxiv.org/pdf/2002.10553\n",
    "    W1=cvx.Variable((d,m1))\n",
    "    W2=cvx.Variable((d,m1))\n",
    "\n",
    "    # parameters\n",
    "    y_out1 = cvx.sum(cvx.multiply(dmat, X@W1),axis=1)\n",
    "    y_out2 = cvx.sum(cvx.multiply(dmat, X@W2),axis=1)\n",
    "\n",
    "    reg_term = cvx.mixed_norm(W1.T, 2, 1) + cvx.mixed_norm(W2.T, 2, 1)\n",
    "    # regularization strength as a cvxpy var\n",
    "    betaval = cvx.Parameter(nonneg=True)\n",
    "\n",
    "    objective_function = cvx.sum(\n",
    "        cvx.sum_squares(y-(y_out1 - y_out2))\n",
    "    ) / n + betaval * reg_term\n",
    "\n",
    "    constraints = [\n",
    "        cvx.multiply(2 * dmat - np.ones((n,m1)), X@W1) >= 0\n",
    "    ] + [\n",
    "        cvx.multiply( 2 * dmat - np.ones((n,m1)), X@W2) >= 0\n",
    "    ]\n",
    "\n",
    "    problem = cvx.Problem(cvx.Minimize(objective_function), constraints)\n",
    "\n",
    "    # Solve the problem for each possible regularization strength\n",
    "    for beta in weight_decay_strength:\n",
    "        print(f\"Trying beta={beta}\")\n",
    "        betaval.value = beta\n",
    "        problem.solve(\n",
    "            solver=cvx.MOSEK,\n",
    "            warm_start=True,\n",
    "            verbose=verbose,\n",
    "            mosek_params=mosek_params\n",
    "        )\n",
    "\n",
    "        print(\"Solution Status: \", problem.status)\n",
    "\n",
    "        W1v=W1.value\n",
    "        W2v=W2.value\n",
    "\n",
    "        ytest_est = np.sum(\n",
    "            drelu(Xtest@U) * (Xtest@W1v) - drelu(Xtest@U) * (Xtest@W2v),\n",
    "            axis=1\n",
    "        )\n",
    "        ytest_est = (ytest_est > 0.5).astype(ytest.dtype)\n",
    "        err = np.sum(ytest_est != ytest) / ntest\n",
    "        print(\"Classification Accuracy\", 1 - err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying beta=0.0\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.7170000000000001\n",
      "Trying beta=1.0\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.736\n",
      "Trying beta=0.5\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.729\n",
      "Trying beta=0.1\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.7070000000000001\n",
      "Trying beta=0.01\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.7090000000000001\n",
      "Trying beta=0.001\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.706\n",
      "Trying beta=0.0001\n",
      "Solution Status:  optimal\n",
      "Classification Accuracy 0.714\n"
     ]
    }
   ],
   "source": [
    "idxs = np.random.choice(X.shape[0], size=1000)\n",
    "solve_problem(X[idxs], y[idxs], Xtest, ytest, \n",
    "              hidden_dim=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
