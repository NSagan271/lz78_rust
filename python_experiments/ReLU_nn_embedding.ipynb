{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from relu_embed.classification import ReLUClassifier, MultiproblemReLUClassifier\n",
    "from relu_embed.data import DatasetInfo, load_and_process_dataset\n",
    "from relu_embed.utils import TokenizerWrapper\n",
    "from relu_embed.embedding import NNEmbedding\n",
    "import mteb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Problem Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "    DatasetInfo(\"mteb/banking77\"),\n",
    "    DatasetInfo(\"mteb/amazon_counterfactual\"),\n",
    "    DatasetInfo(\"mteb/toxic_conversations_50k\"),\n",
    "    DatasetInfo(\n",
    "        \"google-research-datasets/poem_sentiment\", text_column=\"verse_text\"),\n",
    "    DatasetInfo(\n",
    "        \"takala/financial_phrasebank\", name=\"sentences_allagree\",\n",
    "        text_column=\"sentence\", has_splits=False),\n",
    "    DatasetInfo(\"fancyzhx/dbpedia_14\", text_column=\"content\", train_limit=100_000),\n",
    "    DatasetInfo(\"mteb/imdb\"),\n",
    "    DatasetInfo(\"mteb/tweet_sentiment_extraction\"),\n",
    "    \n",
    "]\n",
    "problem_names = [x.dataset for x in problems]\n",
    "\n",
    "n_classes = []\n",
    "Xs, ys, Xtests, ytests, ids = [], [], [], [], []\n",
    "for (i, prob) in enumerate(problems):\n",
    "    X, y, Xtest, ytest = load_and_process_dataset(prob)\n",
    "    n_classes.append(int(torch.max(y).item()) + 1)\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "    Xtests.append(Xtest)\n",
    "    ytests.append(ytest)\n",
    "    ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiproblemReLUClassifier(\n",
    "    n_classes=n_classes, problem_names=problem_names,\n",
    "    input_size=Xs[0].shape[1],\n",
    "    embedding_size=256,\n",
    "    device=\"cuda:7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = model.get_dataloader_batch(\n",
    "    Xs, ys, ids, batch_size=256, normalize_rows=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloaders = [\n",
    "    model.get_dataloader_single(\n",
    "        Xtest, ytest, id_test, batch_size=256, normalize_rows=True\n",
    "    ) for (Xtest, ytest, id_test) in zip(Xtests, ytests, ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    train_dataloader, test_dataloaders,\n",
    "    epochs=5, lr=1e-3,\n",
    "    lr_decay=0.95,\n",
    "    eval_interval=500_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrapper(\"BAAI/bge-base-en-v1.5\")\n",
    "embedding_model = model.get_embedding_model(\n",
    "    tokenizer, normalize_embeds=True).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding_model, \"../object.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = torch.load(\"temp.bin\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.normalize_embeds = True\n",
    "embedding_model.normalize_token_counts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = mteb.get_tasks(tasks=[\"ToxicConversationsClassification\"])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "\n",
    "results = evaluation.run(\n",
    "    embedding_model, output_folder=f\"results/test\",\n",
    "    show_progress_bar=True,\n",
    "    overwrite_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([results[0].scores[name][0][\"main_score\"] * 100 for name in results[0].scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is mostly copied from Mert's Mosek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tokenizer.get_token_count_projection(data[\"train\"][\"text\"], progress=True)\n",
    "# y = np.array(data[\"train\"][\"label\"])\n",
    "# Xtest = tokenizer.get_token_count_projection(data[\"test\"][\"text\"], progress=True)\n",
    "# ytest = np.array(data[\"test\"][\"label\"])\n",
    "\n",
    "# def relu(x):\n",
    "#     return np.maximum(0,x)\n",
    "\n",
    "# def drelu(x):\n",
    "#     return x>=0\n",
    "\n",
    "# # NOTE: Mosek parameter dictionary from Mert\n",
    "# # params = {\n",
    "# #       \"MSK_IPAR_NUM_THREADS\": 8,\n",
    "# #       #\"MSK_IPAR_INTPNT_MAX_ITERATIONS\": 10,\n",
    "# #       #\"MSK_IPAR_OPTIMIZER\": 0 # auto 0, interior point 1, conic 2\n",
    "# #       #\"MSK_DPAR_INTPNT_CO_TOL_REL_GAP\": 1e-2\n",
    "# #       #\"MSK_DPAR_INTPNT_TOL_PSAFE\": 0.01\n",
    "# #       #\"MSK_IPAR_OPTIMIZER\": \"free\"\n",
    "# #       #\"MSK_IPAR_INTPNT_SOLVE_FORM\": 1\n",
    "# #       }\n",
    "# def solve_problem(\n",
    "#     X: np.array, y: np.array, \n",
    "#     Xtest: np.array, ytest: np.array,\n",
    "#     seed=0, hidden_dim=2000,\n",
    "#     weight_decay_strength=None,\n",
    "#     verbose=True,\n",
    "#     mosek_params={\"MSK_IPAR_NUM_THREADS\": 64}\n",
    "# ):\n",
    "#     np.random.seed(seed)\n",
    "#     n,d = X.shape\n",
    "#     ntest = Xtest.shape[0]\n",
    "\n",
    "#     if weight_decay_strength is None:\n",
    "#         weight_decay_strength = np.array([0, 1, 10, 5e-1, 1e-1, 1e-2, 1e-3])\n",
    "\n",
    "#     # Say the two-layer neural network is relu(X @ U1) @ U2.\n",
    "#     # Then, the convex version of the neural network requires\n",
    "#     # knowing all possible formations of indic{X @ U1 > 0}, where\n",
    "#     # the indicator function is taken element-wise. This is\n",
    "#     # computationally expensive, so we estimate it by randomly\n",
    "#     # sampling the matrix U1.\n",
    "#     U1 = np.random.randn(d,hidden_dim)\n",
    "#     dmat = drelu(X @ U1)\n",
    "    \n",
    "#     dmat, ind=(np.unique(dmat,axis=1, return_index=True))\n",
    "#     m1=dmat.shape[1]\n",
    "#     U=U1[:,ind]\n",
    "\n",
    "#     # CVXPY variables for finite-dimensional optimization problem\n",
    "#     # from Section 3 of https://arxiv.org/pdf/2002.10553\n",
    "#     W1=cvx.Variable((d,m1))\n",
    "#     W2=cvx.Variable((d,m1))\n",
    "\n",
    "#     # parameters\n",
    "#     y_out1 = cvx.sum(cvx.multiply(dmat, X@W1),axis=1)\n",
    "#     y_out2 = cvx.sum(cvx.multiply(dmat, X@W2),axis=1)\n",
    "\n",
    "#     reg_term = cvx.mixed_norm(W1.T, 2, 1) + cvx.mixed_norm(W2.T, 2, 1)\n",
    "#     # regularization strength as a cvxpy var\n",
    "#     betaval = cvx.Parameter(nonneg=True)\n",
    "\n",
    "#     objective_function = cvx.sum(\n",
    "#         cvx.sum_squares(y-(y_out1 - y_out2))\n",
    "#     ) / n + betaval * reg_term\n",
    "\n",
    "#     constraints = [\n",
    "#         cvx.multiply(2 * dmat - np.ones((n,m1)), X@W1) >= 0\n",
    "#     ] + [\n",
    "#         cvx.multiply( 2 * dmat - np.ones((n,m1)), X@W2) >= 0\n",
    "#     ]\n",
    "\n",
    "#     problem = cvx.Problem(cvx.Minimize(objective_function), constraints)\n",
    "\n",
    "#     # Solve the problem for each possible regularization strength\n",
    "#     for beta in weight_decay_strength:\n",
    "#         print(f\"Trying beta={beta}\")\n",
    "#         betaval.value = beta\n",
    "#         problem.solve(\n",
    "#             solver=cvx.MOSEK,\n",
    "#             warm_start=True,\n",
    "#             verbose=verbose,\n",
    "#             mosek_params=mosek_params\n",
    "#         )\n",
    "\n",
    "#         print(\"Solution Status: \", problem.status)\n",
    "\n",
    "#         W1v=W1.value\n",
    "#         W2v=W2.value\n",
    "\n",
    "#         ytest_est = np.sum(\n",
    "#             drelu(Xtest@U) * (Xtest@W1v) - drelu(Xtest@U) * (Xtest@W2v),\n",
    "#             axis=1\n",
    "#         )\n",
    "#         ytest_est = (ytest_est > 0.5).astype(ytest.dtype)\n",
    "#         err = np.sum(ytest_est != ytest) / ntest\n",
    "#         print(\"Classification Accuracy\", 1 - err)\n",
    "\n",
    "# idxs = np.random.choice(X.shape[0], size=1000)\n",
    "# solve_problem(X[idxs], y[idxs], Xtest, ytest, \n",
    "#               hidden_dim=200, verbose=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
