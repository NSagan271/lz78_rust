{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import regex as re\n",
    "from model2vec.distill.tokenizer import remove_tokens\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cvx\n",
    "from model2vec import StaticModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example: IMDB Classification\n",
    "\n",
    "### Baselines\n",
    "**Model2Vec**: 65%\n",
    "\n",
    "**Potion**: 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"mteb/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_name: str, potion_model_name: str, device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name, trust_remote_code=True, device=device)\n",
    "        full_vocab = [pair[0] for pair in sorted(self.tokenizer.get_vocab().items(), key=lambda x: x[1])]\n",
    "        vocab = [x for x in full_vocab if not re.match(\"\\[unused\\d+\\]\", x)]\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.tokenizer = remove_tokens(self.tokenizer.backend_tokenizer, set(full_vocab) - set(vocab))\n",
    "        self.tokenizer.no_padding()\n",
    "\n",
    "        model = StaticModel.from_pretrained(potion_model_name)\n",
    "        self.projection = model.embedding\n",
    "\n",
    "    def get_token_counts(self, texts: list[str] | str, progress=False):\n",
    "        if type(texts) == str:\n",
    "            texts = [texts]\n",
    "        encoded = self.tokenizer.encode_batch([x.lower() for x in texts], add_special_tokens=False)\n",
    "        token_counts = np.zeros((len(texts), self.vocab_size))\n",
    "        \n",
    "        iterator = enumerate(encoded)\n",
    "        if progress:\n",
    "            iterator = enumerate(tqdm(encoded))\n",
    "        for (row, enc) in iterator:\n",
    "            for id in enc.ids:\n",
    "                token_counts[row, id] += 1\n",
    "        return token_counts\n",
    "    \n",
    "    def get_token_count_projection(self, texts: list[str] | str, progress=False):\n",
    "        counts = self.get_token_counts(texts, progress=progress)\n",
    "        return counts @ self.projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizerWrapper(\"BAAI/bge-base-en-v1.5\", \"minishlab/potion-base-2M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.get_token_counts(data[\"train\"][\"text\"], progress=True)\n",
    "y = np.array(data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = tokenizer.get_token_counts(data[\"test\"][\"text\"], progress=True)\n",
    "ytest = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Neural Network\n",
    "\n",
    "**The following code should get about 87% accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    model, train, test, loss_fn=None, device=\"cpu\",\n",
    "    epochs=10, lr=0.01, bs=16, weight_decay=0,\n",
    "    lr_decay_coef=0.9\n",
    "):\n",
    "        \n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=bs, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=lr_decay_coef)\n",
    "\n",
    "    test_dataloader = DataLoader(test, batch_size=bs, shuffle=True)\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for (X, y) in test_dataloader:\n",
    "            losses.append(loss_fn(model(X.to(device)),y.to(device)).item())\n",
    "        print(\"Loss\", np.mean(losses))\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "        for (X, y) in train_dataloader:\n",
    "            output = model(X.to(device))\n",
    "            #Compute loss using (Mean Squared Error)\n",
    "            loss = loss_fn(output, y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses = []\n",
    "            for (X, y) in test_dataloader:\n",
    "                losses.append(loss_fn(model(X.to(device)),y.to(device)).item())\n",
    "            print(\"Loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = torch.Tensor(X / np.linalg.norm(X, axis=1, keepdims=True))\n",
    "Xtest_scale = torch.Tensor(Xtest / np.linalg.norm(Xtest, axis=1, keepdims=True))\n",
    "\n",
    "train = TensorDataset(X_scale, torch.Tensor(y).to(torch.long))\n",
    "test = TensorDataset(Xtest_scale, torch.Tensor(ytest).to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(tokenizer.vocab_size, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 2),\n",
    "    nn.Softmax(dim=1),\n",
    ").to(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(\n",
    "    model, train, test,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    device=\"cuda:7\",\n",
    "    epochs=20,\n",
    "    bs=256,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "device = \"cuda:7\"\n",
    "test_dataloader = DataLoader(test, batch_size=256, shuffle=True)\n",
    "for (Xt, yt) in test_dataloader:\n",
    "    losses.append((model(Xt.to(device)).argmax(dim=1) != yt.to(device)).to(torch.float).mean().item())\n",
    "print(\"Classification accuracy:\", 1 - np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with MSE Loss\n",
    "\n",
    "**This should also do about the same, 88%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = torch.Tensor(X / np.linalg.norm(X, axis=1, keepdims=True))\n",
    "Xtest_scale = torch.Tensor(Xtest / np.linalg.norm(Xtest, axis=1, keepdims=True))\n",
    "\n",
    "train = TensorDataset(X_scale, torch.Tensor(y).unsqueeze(1))\n",
    "test = TensorDataset(Xtest_scale, torch.Tensor(ytest).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(tokenizer.vocab_size, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 1),\n",
    ").to(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(\n",
    "    model, train, test,\n",
    "    loss_fn=nn.MSELoss(),\n",
    "    device=\"cuda:7\",\n",
    "    epochs=20,\n",
    "    bs=256,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "device = \"cuda:7\"\n",
    "test_dataloader = DataLoader(test, batch_size=256, shuffle=True)\n",
    "for (Xt, yt) in test_dataloader:\n",
    "    losses.append((torch.round(model(Xt.to(device))) != yt.to(device)).to(torch.float).mean().item())\n",
    "print(\"Classification accuracy:\", 1 - np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Sentiment Classification (potion: 55.4%)\n",
    "\n",
    "Here, cross-entropy ends up doing better than MSE (maybe because it's multiclass).\n",
    "\n",
    "**The following code should get about 70% accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.get_token_counts(data[\"train\"][\"text\"], progress=True)\n",
    "y = np.array(data[\"train\"][\"label\"])\n",
    "Xtest = tokenizer.get_token_counts(data[\"test\"][\"text\"], progress=True)\n",
    "ytest = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = torch.Tensor(X / np.maximum(1, np.linalg.norm(X, axis=1, keepdims=True)))\n",
    "Xtest_scale = torch.Tensor(Xtest / np.maximum(1, np.linalg.norm(Xtest, axis=1, keepdims=True)))\n",
    "\n",
    "train = TensorDataset(X_scale, torch.Tensor(y).to(torch.long))\n",
    "test = TensorDataset(Xtest_scale, torch.Tensor(ytest).to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(tokenizer.vocab_size, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, 3),\n",
    "    nn.Softmax(dim=1),\n",
    ").to(\"cuda:7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(\n",
    "    model, train, test,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    device=\"cuda:7\",\n",
    "    epochs=20,\n",
    "    bs=256,\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "device = \"cuda:7\"\n",
    "test_dataloader = DataLoader(test, batch_size=256, shuffle=True)\n",
    "for (Xt, yt) in test_dataloader:\n",
    "    losses.append((model(Xt.to(device)).argmax(dim=1) != yt.to(device)).to(torch.float).mean().item())\n",
    "print(\"Classification accuracy:\", 1 - np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is mostly copied from Mert's Mosek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.get_token_count_projection(data[\"train\"][\"text\"], progress=True)\n",
    "y = np.array(data[\"train\"][\"label\"])\n",
    "Xtest = tokenizer.get_token_count_projection(data[\"test\"][\"text\"], progress=True)\n",
    "ytest = np.array(data[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def drelu(x):\n",
    "    return x>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Mosek parameter dictionary from Mert\n",
    "# params = {\n",
    "#       \"MSK_IPAR_NUM_THREADS\": 8,\n",
    "#       #\"MSK_IPAR_INTPNT_MAX_ITERATIONS\": 10,\n",
    "#       #\"MSK_IPAR_OPTIMIZER\": 0 # auto 0, interior point 1, conic 2\n",
    "#       #\"MSK_DPAR_INTPNT_CO_TOL_REL_GAP\": 1e-2\n",
    "#       #\"MSK_DPAR_INTPNT_TOL_PSAFE\": 0.01\n",
    "#       #\"MSK_IPAR_OPTIMIZER\": \"free\"\n",
    "#       #\"MSK_IPAR_INTPNT_SOLVE_FORM\": 1\n",
    "#       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(\n",
    "    X: np.array, y: np.array, \n",
    "    Xtest: np.array, ytest: np.array,\n",
    "    seed=0, hidden_dim=2000,\n",
    "    weight_decay_strength=None,\n",
    "    verbose=True,\n",
    "    mosek_params={\"MSK_IPAR_NUM_THREADS\": 64}\n",
    "):\n",
    "    np.random.seed(seed)\n",
    "    n,d = X.shape\n",
    "    ntest = Xtest.shape[0]\n",
    "\n",
    "    if weight_decay_strength is None:\n",
    "        weight_decay_strength = np.array([0, 1, 10, 5e-1, 1e-1, 1e-2, 1e-3])\n",
    "\n",
    "    # Say the two-layer neural network is relu(X @ U1) @ U2.\n",
    "    # Then, the convex version of the neural network requires\n",
    "    # knowing all possible formations of indic{X @ U1 > 0}, where\n",
    "    # the indicator function is taken element-wise. This is\n",
    "    # computationally expensive, so we estimate it by randomly\n",
    "    # sampling the matrix U1.\n",
    "    U1 = np.random.randn(d,hidden_dim)\n",
    "    dmat = drelu(X @ U1)\n",
    "    \n",
    "    dmat, ind=(np.unique(dmat,axis=1, return_index=True))\n",
    "    m1=dmat.shape[1]\n",
    "    U=U1[:,ind]\n",
    "\n",
    "    # CVXPY variables for finite-dimensional optimization problem\n",
    "    # from Section 3 of https://arxiv.org/pdf/2002.10553\n",
    "    W1=cvx.Variable((d,m1))\n",
    "    W2=cvx.Variable((d,m1))\n",
    "\n",
    "    # parameters\n",
    "    y_out1 = cvx.sum(cvx.multiply(dmat, X@W1),axis=1)\n",
    "    y_out2 = cvx.sum(cvx.multiply(dmat, X@W2),axis=1)\n",
    "\n",
    "    reg_term = cvx.mixed_norm(W1.T, 2, 1) + cvx.mixed_norm(W2.T, 2, 1)\n",
    "    # regularization strength as a cvxpy var\n",
    "    betaval = cvx.Parameter(nonneg=True)\n",
    "\n",
    "    objective_function = cvx.sum(\n",
    "        cvx.sum_squares(y-(y_out1 - y_out2))\n",
    "    ) / n + betaval * reg_term\n",
    "\n",
    "    constraints = [\n",
    "        cvx.multiply(2 * dmat - np.ones((n,m1)), X@W1) >= 0\n",
    "    ] + [\n",
    "        cvx.multiply( 2 * dmat - np.ones((n,m1)), X@W2) >= 0\n",
    "    ]\n",
    "\n",
    "    problem = cvx.Problem(cvx.Minimize(objective_function), constraints)\n",
    "\n",
    "    # Solve the problem for each possible regularization strength\n",
    "    for beta in weight_decay_strength:\n",
    "        print(f\"Trying beta={beta}\")\n",
    "        betaval.value = beta\n",
    "        problem.solve(\n",
    "            solver=cvx.MOSEK,\n",
    "            warm_start=True,\n",
    "            verbose=verbose,\n",
    "            mosek_params=mosek_params\n",
    "        )\n",
    "\n",
    "        print(\"Solution Status: \", problem.status)\n",
    "\n",
    "        W1v=W1.value\n",
    "        W2v=W2.value\n",
    "\n",
    "        ytest_est = np.sum(\n",
    "            drelu(Xtest@U) * (Xtest@W1v) - drelu(Xtest@U) * (Xtest@W2v),\n",
    "            axis=1\n",
    "        )\n",
    "        ytest_est = (ytest_est > 0.5).astype(ytest.dtype)\n",
    "        err = np.sum(ytest_est != ytest) / ntest\n",
    "        print(\"Classification Accuracy\", 1 - err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.choice(X.shape[0], size=1000)\n",
    "solve_problem(X[idxs], y[idxs], Xtest, ytest, \n",
    "              hidden_dim=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
