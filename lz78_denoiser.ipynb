{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LZ78 Python Interface for Denoising with Lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Remember to do `cd crates/python/ && maturin develop && cd ../..` from the `lz78_rust` directory after making any changes to the Rust or the Python interface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sys import stdout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.signal as signal\n",
    "from scipy.linalg import toeplitz\n",
    "import dill as pickle\n",
    "from lz78 import Sequence, LZ78Encoder, CharacterMap, BlockLZ78Encoder, LZ78SPA\n",
    "from lz78 import encoded_sequence_from_bytes, spa_from_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Usage Example\n",
    "This is equivalent to the traversal and lookahead tests in `spa.rs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17043380775717953"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa = LZ78SPA(2, gamma=0.5)\n",
    "input = Sequence([1, 0] * 1000, alphabet_size=2)\n",
    "train_loss = spa.train_on_block(input) / 2000\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9883720930232558, 0.011627906976744186]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_traverse = spa.traverse_and_get_prob([0, 1])\n",
    "pdf_traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_lookahead = spa.traverse_and_get_prob_with_lookahead(input=[1, 0, 1], lookahead=[1, 0])\n",
    "pdf_lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_denoiser_naive(signal_to_filter, spa_tree, char_map, window_len,pi_matrix, x_alphabet):\n",
    "    sequence_l = len(signal_to_filter)\n",
    "    \n",
    "    Zt_index = signal_to_filter[window_len:sequence_l]\n",
    "    print(Zt_index[:10  ])\n",
    "    filtered_signal = []\n",
    "    for il in range(sequence_l-window_len):\n",
    "        prob = spa_tree.traverse_and_get_prob(signal_to_filter[il:il+window_len])\n",
    "        PX_t_Z_tm1_ele=np.matmul(np.linalg.inv(np.transpose(pi_matrix)),prob)\n",
    "        alphabet_ind = Zt_index[il]\n",
    "        num = pi_matrix[:,alphabet_ind] * PX_t_Z_tm1_ele\n",
    "        den = prob[alphabet_ind]\n",
    "        pxt = num/den\n",
    "        pxt = pxt/np.sum(pxt)\n",
    "        xt_hat = np.dot(pxt,x_alphabet)\n",
    "        filtered_signal.append(xt_hat)\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "def get_prob_from_MC(Dd, seed_str, spa_tree, char_map,num_exper):\n",
    "    generated_seq = []\n",
    "    for i_mc in range(num_exper):\n",
    "        output, loss = spa_tree.generate_data(Dd, seed_data = seed_str,temperature =1, min_context= 3)\n",
    "        generated_seq.append(output)\n",
    "    # count the number of times of last element in the generated sequence\n",
    "    prob = np.zeros(3)\n",
    "    for i in range(num_exper):\n",
    "        # charmap the generated sequence\n",
    "        generated_ind = char_map.str_to_symbol_list(generated_seq[i][-1])\n",
    "        #print(generated_ind)    \n",
    "        prob[generated_ind] += 1\n",
    "    prob = prob/num_exper\n",
    "    return prob\n",
    "\n",
    "def universal_denoiser_delay_mc(Delay,signal_to_filter_mc, spa_tree, char_map, \n",
    "                                window_len,pi_matrix, x_alphabet, num_exp):\n",
    "    sequence_len = len(signal_to_filter_mc) \n",
    "    Zt_index = char_map.str_to_symbol_list(signal_to_filter_mc[window_len:sequence_len])\n",
    "    filtered_signal_delay_mc = []\n",
    "    for idmc in range(sequence_len-window_len):\n",
    "        Zt_seed_str = signal_to_filter_mc[idmc:window_len+idmc-Delay]\n",
    "        prob_mc = get_prob_from_MC(Delay, Zt_seed_str, spa_tree, num_exp)\n",
    "        \n",
    "        PX_t_Z_tm1_mc = np.matmul(np.linalg.inv(np.transpose(pi_matrix)),prob_mc)\n",
    "        alphabet_ind = Zt_index[idmc]\n",
    "        #print(alphabet_ind)\n",
    "        num = pi_matrix[:,alphabet_ind] * PX_t_Z_tm1_mc\n",
    "        den = prob_mc[alphabet_ind]\n",
    "        pxt = num/den\n",
    "        pxt = pxt/np.sum(pxt)\n",
    "        xt_hat_mc = np.dot(pxt,x_alphabet)\n",
    "        #print(xt_hat_mc)\n",
    "        filtered_signal_delay_mc.append(xt_hat_mc)\n",
    "    \n",
    "    #print(filtered_signal_delay_mc)\n",
    "\n",
    "    return filtered_signal_delay_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_denoiser_lookahead(Lookahead,signal_to_filter_la, spa_tree, char_map: CharacterMap, \n",
    "                                window_len,pi_matrix, x_alphabet):\n",
    "\n",
    "    sequence_len = len(signal_to_filter_la) \n",
    "    Zt_index =signal_to_filter_la[window_len:sequence_len]\n",
    "    \n",
    "    filtered_signal_lookahead = []\n",
    "    #signal_to_filter_la = np.array(signal_to_filter_la)\n",
    "    for ila in range(sequence_len-window_len):\n",
    "        Zt_seed_str = signal_to_filter_la[ila:window_len+ila]\n",
    "        Zt_lookahead_str = signal_to_filter_la[ila+window_len:ila+window_len+Lookahead]\n",
    "\n",
    "        Zt_seed_vec = Zt_seed_str\n",
    "        Zt_lookahead_vec = Zt_lookahead_str\n",
    "        prob_la = spa_tree.traverse_and_get_prob_with_lookahead(Zt_seed_vec, Zt_lookahead_vec)\n",
    "        \n",
    "        PX_t_Z_tm1_la = np.matmul(np.linalg.inv(np.transpose(pi_matrix)),prob_la)\n",
    "        alphabet_ind = Zt_index[ila]\n",
    "        num = pi_matrix[:,alphabet_ind] * PX_t_Z_tm1_la\n",
    "        den = prob_la[alphabet_ind]\n",
    "        pxt = num/den\n",
    "        pxt = pxt/np.sum(pxt)\n",
    "        xt_hat_la = np.dot(pxt,x_alphabet)\n",
    "        filtered_signal_lookahead.append(xt_hat_la)\n",
    "    \n",
    "    return filtered_signal_lookahead    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markov_sequence_random_initial(N, p):\n",
    "    \"\"\"\n",
    "    Generates a Markov sequence of length N with two states: -1 and 1.\n",
    "    The initial state is randomly chosen with a 50:50 chance.\n",
    "    The probability of switching states is p, and the probability of staying in the same state is 1 - p.\n",
    "    \n",
    "    Parameters:\n",
    "    N (int): Length of the Markov sequence.\n",
    "    p (float): Probability of switching states.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The generated Markov sequence.\n",
    "    \"\"\"\n",
    "    # Initialize the first state randomly with a 50:50 chance\n",
    "    initial_state = np.random.choice([1, -1])\n",
    "    \n",
    "    # Initialize the sequence\n",
    "    sequence = np.zeros(N)\n",
    "    sequence[0] = initial_state\n",
    "\n",
    "    # Generate the Markov sequence\n",
    "    for i in range(1, N):\n",
    "        if np.random.rand() < p:\n",
    "            sequence[i] = -sequence[i-1]  # Switch state\n",
    "        else:\n",
    "            sequence[i] = sequence[i-1]  # Stay in the same state\n",
    "\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this initializes the sequence that trains the SPA\n",
    "#N = 1_000_000_000   #--> between 3^18 and 3^19  \n",
    "N =  1_000_000_000 \n",
    "D = 2\n",
    "p = 0.8 # probability of changing state\n",
    "Xt= generate_markov_sequence_random_initial(N, p)\n",
    "Nt = np.random.choice([-1, 1], size=N, p=[0.5, 0.5])\n",
    "Zt=  Xt + Nt + 2\n",
    "# convert Zt to integers\n",
    "Zt = Zt.astype(int)\n",
    "Zt_str = ''.join(str(num) for num in Zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4459596367035228"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush() ## add for any long process\n",
    "spa = LZ78SPA(3, gamma=0.5)\n",
    "charmap_str = \"000022224444\"  ## IMPORTANT: do specify the sequence of the alphabet you want to train on. I'll make this more robust in the next update.\n",
    "char_map =  CharacterMap(charmap_str)\n",
    "\n",
    "#train on shifted input\n",
    "for shift in range(5):\n",
    "    input = Sequence(Zt_str[shift:], charmap =char_map)\n",
    "    train_loss = spa.train_on_block(input) / N\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "bytes = spa.to_bytes()\n",
    "\n",
    "with open(\"test_data/trained_markov_spa.bin\", 'wb') as file:\n",
    "    file.write(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained spa\n",
    "with open(\"trained_markov_spa.bin\", 'rb') as file:\n",
    "    spa = spa_from_bytes(file.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0. ]\n",
      " [0.5 0.  0.5]\n",
      " [0.  0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "pi_matrix_true = np.array([[0.5,0.5,0],[1/2,0,1/2],[0, 0.5,0.5]])\n",
    "print(pi_matrix_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_alphabet = input.alphabet_size() # 3\n",
    "pi_matrix_approx = np.zeros((len_alphabet, len_alphabet))\n",
    "# approximate the transition matrix from Xt to Zt\n",
    "alphabet_Xt = [-1,0, 1]\n",
    "\n",
    "def compute_pi_matrix(Xt,Zt_str,alphabet, alphabet_Xt):\n",
    "    pi_matrix = np.ones((len(alphabet_Xt), len(alphabet)))\n",
    "    for i in range(len(alphabet)):\n",
    "        for j in range(len(alphabet)):\n",
    "            # count the number of times alphabet[i] is followed by alphabet[j]\n",
    "            count = 1\n",
    "            for k in range(len(Xt)-1):\n",
    "                if Xt[k] == alphabet_Xt[i] and Zt_str[k] == alphabet[j]:\n",
    "                    count += 1\n",
    "            pi_matrix[i,j] = count\n",
    "    \n",
    "    pi_matrix = pi_matrix / np.sum(pi_matrix, axis=1)[:,np.newaxis]\n",
    "    return pi_matrix\n",
    "pi_matrix_approx = compute_pi_matrix(Xt,Zt_str,['-2','0','2'], alphabet_Xt)\n",
    "pi_matrix_approx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220220202224202020402222402042020442240022242240042204242242004002022224020240204224222004004220402240402424422240204220422202024022404240404220222204020224022222240220422242442424200202202404422220204422024224022224042422040222224022402222022224240204240422240224240222424024204220402024224222422202242442040222224202220424022224424022204020224202222200422042402402424222002022404202202204220420024220222042240222242422220222240242020420442440202024224240400404240424220220422020220222040202202020204022022224222404224220222220222020422220222200420422042422202240224242422020402220240224022224022404220404222424004222420440242004404022222200002040220420022020402240222202424202242424222240222242424240220422404420042404224240424222240402440222402240222002422240224222240220202220204022422402202220420240200222422004022240404242242222224222222224422224220204220220440424040224222024022042040240402204004242022424242404222202224220424242200404242420422242202242040404202404004224224200202040420222242\n",
      "[1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 2, 2, 1, 1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1, 1, 2, 0, 2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 2, 2, 1, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2, 0, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 0, 2, 1, 1, 0, 2, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 2, 1, 2, 0, 1, 2, 0, 1, 2, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 2, 2, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 1, 2, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 2, 1, 0, 2, 1, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 2, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 1, 2, 0, 0, 2, 1, 1, 1, 2, 1, 0, 2, 2, 0, 1, 2, 1, 0, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, 2, 0, 1, 1, 1, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 2, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 1, 2, 0, 2, 0, 1, 2, 2, 0, 1, 1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 1, 0, 0, 1, 2, 1, 1, 1, 2, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 2, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 1, 2, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 2, 1, 1, 0, 1, 1, 0, 2, 2, 0, 2, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 1, 0, 2, 1, 0, 2, 0, 1, 2, 0, 2, 0, 1, 1, 0, 2, 0, 0, 2, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 0, 2, 1, 2, 1, 2, 1, 0, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 0, 2, 0, 2, 0, 2, 1, 0, 1, 2, 0, 2, 0, 0, 2, 1, 1, 2, 1, 1, 2, 1, 0, 0, 1, 0, 1, 0, 2, 0, 2, 1, 0, 1, 1, 1, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "stdout.flush()\n",
    "LAH =1# lookahead\n",
    "WL = 5 # window length\n",
    "start = 11000\n",
    "stop_seq = 12000\n",
    "alphabet_Xt = [-1,0, 1]\n",
    "print(Zt_str[start:stop_seq])\n",
    "Zt_to_filter = char_map.encode(Zt_str[start:stop_seq])\n",
    "print(Zt_to_filter)\n",
    "#Zt_encoded = char_map.encode(\"\".join(Zt_to_filter))\n",
    "#Xt_hat = universal_denoiser_naive(Zt_to_filter, spa, char_map,WL, pi_matrix_true, alphabet_Xt)\n",
    "Xt_hat = universal_denoiser_lookahead(LAH,Zt_to_filter, spa, char_map,WL, pi_matrix_true, alphabet_Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41046149000949395"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean = Xt[start+WL:stop_seq]\n",
    "# calculate the mean squared error on not-nan values\n",
    "MSE_loss = 0 \n",
    "k = 0\n",
    "for i in range(len(Xt_hat)):\n",
    "    if not np.isnan(Xt_hat[i]):\n",
    "        loss_ind = (Xt_hat[i] - X_clean[i])**2\n",
    "        if loss_ind <4:\n",
    "            MSE_loss += loss_ind\n",
    "            k += 1\n",
    "        else:\n",
    "            print(Xt_hat[i], X_clean[i])\n",
    "#MSE_loss = MSE_loss/X_clean.shape[0]\n",
    "MSE_loss = MSE_loss/k\n",
    "MSE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0028757880765402, 0.9988846132396411, -1.0053230031770806, 0.3068248801865758, -0.9895710928319624, 0.4358658621680418, -0.0024763698510690824, 1.0093524049041178, -0.777767822477078, -0.9371284185493458, 0.9919210053859961, -0.9973632665360855, 0.7824819781412293, 1.0152662432828525, -1.0310421286031044, 0.9946106381316879, -0.7195978929073374, 0.6372583326043216, 1.0219711236660385, -0.7242989047266974, 0.640334058797112, 0.9729638701775872, -0.9871811306242789, 0.5130308116153139, -0.15328889871645024, -0.1553875458367247, -1.0128057806170148, 0.6017389410813163, -0.3615734679882801, 0.21192143560564608, 0.0006612817585427422, 0.0006612817585427422, 0.07854639379782236, -0.1312643570390512, 0.21612840466926064, -0.35485847465185816, -1.0128057806170148, 0.5464994435825161, -0.24353331631350533, -0.0010511118990932933, 0.3606855894202536, 1.0031583380123525, -1.001634375371449, 1.013627465743111, -1.0069665752612467, 0.9979972904517876, -1.0061811534946368, 1.0057759247217228, -0.6016182683436118, 0.29202612029714836, -0.09044824172857524, -0.21845418574884046, 0.3614325922983388, 0.9979802373997888, -1.005226696356776, 0.7044722048719552, -0.6311573259612969, -1.024995463618218, 0.30788119100020017, -0.9913255799010473, 1.0, -0.3021991767301177, 1.0008484162895928, -0.9931501196665842, 0.3064601471932273, -1.0125406245584287, 0.9925759668508287, -1.0035680304471932, 0.9999409123138738, -0.7917772016057181, -1.0063282525857369, 0.5271202236719479, -0.15435107668133785, -0.1553875458367247, -1.0128057806170148, 0.6453670339102191, -0.4699965236943513, 0.41318189256333593, -0.36059207006558736, -1.0114487282628877, 0.72277363857451, -0.6348463645183169, -1.001478287650138, 1.0007818417259156, -0.5452898921474325, 0.23775937851374823, 0.0004163197335553259, -0.3533499325804219, -1.0048679347336158, 1.003742389543652, -0.30681091141743727, 1.0050140056022407, -0.9975896085345711, 0.9974114503469111, -0.602336540946474, 0.36048950433406435, -0.21604796704527413]\n"
     ]
    }
   ],
   "source": [
    "print(Xt_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1., -1., -1.,  1., -1.,  1., -1.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 2, 2, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "stdout.flush()\n",
    "LAH =2 # lookahead\n",
    "WL = 13 # window length\n",
    "alphabet_Xt = [-1,0, 1]\n",
    "#print(Zt_str[11000:12000])\n",
    "Zt_to_filter = char_map.encode(Zt_str[110000:120000])\n",
    "#print(Zt_to_filter)\n",
    "#Zt_encoded = char_map.encode(\"\".join(Zt_to_filter))\n",
    "Xt_hat = universal_denoiser_naive(Zt_to_filter, spa, char_map,WL, pi_matrix_true, alphabet_Xt)\n",
    "#Xt_hat = universal_denoiser_lookahead(LAH,Zt_to_filter, spa, char_map,WL, pi_matrix_true, alphabet_Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4207241235836166"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean = Xt[110000+WL:120000]\n",
    "# calculate the mean squared error on not-nan values\n",
    "MSE_loss = 0 \n",
    "k = 0\n",
    "for i in range(len(Xt_hat)):\n",
    "    if not np.isnan(Xt_hat[i]):\n",
    "        loss_ind = (Xt_hat[i] - X_clean[i])**2\n",
    "\n",
    "        MSE_loss += loss_ind\n",
    "        \n",
    "MSE_loss = MSE_loss/X_clean.shape[0]\n",
    "\n",
    "MSE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
