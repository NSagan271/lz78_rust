{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import Sequence, LZ78Encoder, CharacterMap, BlockLZ78Encoder, LZ78SPA\n",
    "from lz78 import encoded_sequence_from_bytes, spa_from_bytes\n",
    "import numpy as np\n",
    "import lorem\n",
    "import requests\n",
    "from sys import stdout\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LZ78 Sequential Probability Assignment (SPA)\n",
    "The `LZ78SPA` class is the implementation of the family of sequential probability assignments discussed in [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589), for Dirichelt priors.\n",
    "In this section, `gamma` refers to the Dirichlet parameter.\n",
    "\n",
    "Under this prior, the sequential probability assignment is an additive\n",
    "perturbation of the emprical distribution, conditioned on the LZ78 prefix\n",
    "of each symbol (i.e., the probability model is proportional to the\n",
    "number of times each node of the LZ78 tree has been visited, plus gamma).\n",
    "\n",
    "This SPA has the following capabilities:\n",
    "- training on one or more sequences,\n",
    "- log loss (\"perplexity\") computation for test sequences,\n",
    "- SPA computation (using the LZ78 context reached at the end of parsing\n",
    "    the last training block),\n",
    "- sequence generation.\n",
    "\n",
    "Note that the LZ78SPA does not perform compression; you would have to use\n",
    "a separate BlockLZ78Encoder object to perform block-wise compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example: LZ78 SPA on Markov Data\n",
    "\n",
    "We will use the Markov probability source used in [(Rajaraman et al, 2024)](https://arxiv.org/pdf/2404.08335), where the transition probability depends solely on $x_{t-k}$.\n",
    "Specifically, $x_t = x_{t-k}$ with probability $0.9$, and otherwise $x_t$ is picked uniformly at random from the rest of the alphabet.\n",
    "\n",
    "The SPA works best when the alphabet size is $2$, but you can try out other alphabet sizes too.\n",
    "\n",
    "First, we define some helper functions for generating the data (don't worry about understanding these; they are irrelevant to understanding the SPA itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for generating data; feel free run the cell without\n",
    "# reading the code\n",
    "def sample_index_from_dist(probabilities):\n",
    "    cdf = np.cumsum(probabilities)\n",
    "    cdf[-1] = 1 # in case of FP error\n",
    "    return int(np.where(np.random.random() < cdf)[0][0])\n",
    "\n",
    "def entropy(probs):\n",
    "    return sum([-x * np.log2(x) for x in probs if x > 0])\n",
    "\n",
    "def get_stationary_dist(transition_probabilities):\n",
    "    eigvals, eigvecs = np.linalg.eig(transition_probabilities.T)\n",
    "    # all eigenvalues will be <= 1, and one will be =1\n",
    "    stationary_dist = eigvecs[:, np.argmax(eigvals)]\n",
    "    return stationary_dist / sum(stationary_dist)\n",
    "\n",
    "def entropy_rate(transition_probabilities):\n",
    "    stationary_dist = get_stationary_dist(transition_probabilities)\n",
    "    return sum([prob * entropy(transition_probabilities[i]) \n",
    "                for i, prob in enumerate(stationary_dist)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data to pass through the SPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can change these\n",
    "ALPHABET_SIZE = 2\n",
    "PEAK_PROB = 0.9\n",
    "K = 5\n",
    "N = 1_000_000\n",
    "N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data array; feel free to ignore this code and just run the cell\n",
    "transition_probabilities = np.eye(ALPHABET_SIZE) * PEAK_PROB + \\\n",
    "    (np.ones((ALPHABET_SIZE, ALPHABET_SIZE)) - np.eye(ALPHABET_SIZE)) * (1 - PEAK_PROB) / (ALPHABET_SIZE - 1)\n",
    "start_prob = np.ones(ALPHABET_SIZE) / ALPHABET_SIZE\n",
    "\n",
    "data = np.zeros(N, dtype=int)\n",
    "for i in range(K):\n",
    "    data[i] = sample_index_from_dist(start_prob)\n",
    "for i in range(K,N):\n",
    "    data[i] = sample_index_from_dist(transition_probabilities[data[i-K]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Sequence(data[:-N_TEST], alphabet_size=ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `train_on_block`\n",
    "\n",
    "Use a block of data to update the SPA. If `include_prev_context` is\n",
    "true, then this block is considered to be from the same sequence as\n",
    "the previous. Otherwise, it is assumed to be a separate sequence, and\n",
    "we return to the root of the LZ78 prefix tree.\n",
    "\n",
    "It returns the self-entropy log loss incurred while processing this\n",
    "sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = LZ78SPA(ALPHABET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7434136165483234"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa.train_on_block(sequence) / (N - N_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `compute_test_loss`\n",
    "After training a SPA, you can compute the log loss of a test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6337562612642951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa.compute_test_loss(Sequence(data[-N_TEST:], alphabet_size=ALPHABET_SIZE)) / N_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_normalized_log_loss`\n",
    "Gets the normaliized self-entropy log loss incurred from training the SPA thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7434136165483234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa.get_normalized_log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `compute_spa_at_current_state`\n",
    "Computes the SPA for every symbol in the alphabet, using the LZ78 context reached at the end of parsing the last training block.\n",
    "\n",
    "In this case, the method will return a two-element list, where the first element is the estimated probability that the next symbol is $0$ and the second is the estimated probability that the next symbol is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07110609480812641, 0.9288939051918735]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One component \"should\" be 0.9 and the other \"should\" be 0.1, but this is\n",
    "# not necessarily the case. e.g., if we are at the top of the LZ78 prefix tree\n",
    "# or at a leaf, we can expect the SPA to be closer to [0.5, 0.5]\n",
    "spa.compute_spa_at_current_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `to_bytes`\n",
    "This works the same as the corresponding index of `CompressedSequence`; refer to the LZ78 Encoding part of the tutorial for more details.\n",
    "\n",
    "The method `spa_from_bytes` reconstructs a SPA from its `bytes` representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance method: `reset_state`\n",
    "\n",
    "Reset the state of the LZ78 prefix tree to the root. This can be called,\n",
    "e.g., between training on two sequences that should be treated as separate\n",
    "sequences.\n",
    "\n",
    "This replaces the parameter `include_prev_context` for the SPA training and test functions in the first iteration of this library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example: Text Generation\n",
    "\n",
    "Let's use the LZ78 SPA to generate some text based on Sherlock Holmes novels.\n",
    "\n",
    "This requires the `requests` library and an internet connection.\n",
    "If you don't have either, you can perform the same experiment any text you'd like, including the lorem ipsum text from the beginning of this tutorial.\n",
    "Just make sure you have enough training data (e.g., the Sherlock novel used for this example is 500 kB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import Sequence, LZ78Encoder, CharacterMap, BlockLZ78Encoder, LZ78SPA\n",
    "from lz78 import encoded_sequence_from_bytes, spa_from_bytes\n",
    "import numpy as np\n",
    "import lorem\n",
    "import requests\n",
    "from sys import stdout\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(\"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own character map and filter the text based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ,?\\n\\\"';:\\t-_\")\n",
    "filtered_text = charmap.filter_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the SPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.784460934046932"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa = LZ78SPA(charmap.alphabet_size(), gamma=0.2)\n",
    "spa.train_on_block(Sequence(filtered_text, charmap=charmap)) / len(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa.set_inference_params(\n",
    "    gamma = 0.2,\n",
    "    lb=1e-5,\n",
    "    lb_or_temp_first=\"lb_first\",\n",
    "    ensemble_type=\"depth\",\n",
    "    ensemble_n=6,\n",
    "    backshift_parsing=True,\n",
    "    backshift_ctx_len=6,\n",
    "    backshift_min_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.995425013825895"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdout.flush()\n",
    "spa.reset_state()\n",
    "spa.compute_test_loss(Sequence(filtered_text, charmap=charmap)) / len(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `generate_data`\n",
    "Generates a sequence of data, using temperature and top-k sampling (see\n",
    "the \"Experiments\" section of [Sagan and Weissman 2024] for more details).\n",
    "\n",
    "Inputs:\n",
    "- **len**: number of symbols to generate\n",
    "- **seed_data**: you can specify that the sequence of generated data\n",
    "    be the continuation of the specified sequence.\n",
    "- **temperature**: a measure of how \"random\" the generated sequence is. A\n",
    "    temperature of 0 deterministically generates the most likely\n",
    "    symbols, and a temperature of 1 samples directly from the SPA.\n",
    "    Temperature values around 0.1 or 0.2 function well.\n",
    "- **top_k**: forces the generated symbols to be of the top_k most likely\n",
    "    symbols at each timestep.\n",
    "- **desired_context_length**: the SPA tries to maintain a context of at least a\n",
    "    certain length at all times. So, when we reach a leaf of the LZ78\n",
    "    prefix tree, we try traversing the tree with different suffixes of\n",
    "    the generated sequence until we get a sufficiently long context\n",
    "    for the next symbol.\n",
    "- **min_spa_training_points**: requires that a node of the LZ78 prefix tree\n",
    "    has been visited at least this number of times during training before\n",
    "    it can be used for generation. i.e., instead of returning to the\n",
    "    root upon reaching a leaf, we would return to the root once we reach\n",
    "    any node that has not been traversed enough times.\n",
    "\n",
    "Returns a tuple of the generated sequence and that sequence's log loss,\n",
    "or perplexity.\n",
    "\n",
    "Errors if the SPA has not been trained so far, or if the seed data is\n",
    "not over the same alphabet as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa.set_generation_params(\n",
    "    gamma = 0.2,\n",
    "    ensemble_type=\"disabled\",\n",
    "    # ensemble_n=6,\n",
    "    backshift_parsing=True,\n",
    "    backshift_ctx_len=20,\n",
    "    backshift_min_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "(generated, loss) = spa.generate_data(\n",
    "    500,\n",
    "    seed_data=Sequence(\"This \", charmap=charmap),\n",
    "    temperature=0.2,\n",
    "    top_k=5,\n",
    ")\n",
    "generated = generated.get_data()\n",
    "generated = \"This \" + generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(generated, max_line_width=80):\n",
    "    for line in generated.split(\"\\n\"):\n",
    "        line_width = 0\n",
    "        for line2 in line.split(\"\\t\"):\n",
    "            for word in line2.split(\" \"):\n",
    "                if line_width + len(word) + 1 > max_line_width:\n",
    "                    line_width = len(word)\n",
    "                    print()\n",
    "                    while line_width > max_line_width:\n",
    "                        line_width -= max_line_width\n",
    "                        print(word[:max_line_width])\n",
    "                        word = word[max_line_width]                        \n",
    "                else:\n",
    "                    line_width += len(word)\n",
    "                print(word, end=\" \")\n",
    "            if line_width + 8 <= max_line_width:\n",
    "                line_width += 8\n",
    "                print(\"\\t\", end=\"\")\n",
    "        print()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is money was morning little to _e and a should be exposure is an orprise, and the thing \n",
      "behind he shot distribution is a man with a week a little daughter client he has dressed, and his \n",
      "who has we dried the very business as amust comply \t\n",
      "heard a little dark profession. He was sob in the sum from the road is forced by the ashed back \n",
      "turned over. \t\n",
      " \t\n",
      "Well, it is \t\n",
      "nally expense \t\n",
      "to the stablish could not the sitting red \t\n",
      "with the office been so that I feel to find a take a little George But if you would n \t\n"
     ]
    }
   ],
   "source": [
    "pretty_print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
