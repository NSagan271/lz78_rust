{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LZ78 Usage Tutorial: Probability Source\n",
    "\n",
    "## Prerequisites\n",
    "1. Follow the setup instructions in `tutorials/README.md`\n",
    "2. In the same Python environment as you used for that tutorial, run `pip install ipykernel`\n",
    "3. Use that Python environment as the kernel for this notebook.\n",
    "\n",
    "## Important Note\n",
    "Sometimes, Jupyter doesn't register that a cell containing code from the `lz78` library has started running, so it seems like the cell is waiting to run until it finishes.\n",
    "This can be annoying for operations that take a while to run, and **can be remedied by putting `stdout.flush()` at the beginning of the cell**.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78 import DirichletLZ78Source, DiracDirichletLZ78Source, DiscreteThetaLZ78Source, mu_k\n",
    "from sys import stdout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import gamma as GammaFunc\n",
    "import scipy.special as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LZ78 Probability Source\n",
    "\n",
    "[The LZ78 Source](https://arxiv.org/abs/2503.10574) describes a probability source that draws symbols from the sequential probability assignment of [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589), starting with just a single root node and traversing/building the tree as symbols are drawn.\n",
    "\n",
    "[The LZ78 Source](https://arxiv.org/abs/2503.10574) proves results on:\n",
    "1. the entropy rate of the source,\n",
    "2. convergence of a realization's log probability to the entropy rate, and \n",
    "3. the almost-sure convergence of any fixed-order empirical entropy (of a realization) to a quantity that is a \"Jensen gap\" larger than the entropy rate.\n",
    "\n",
    "This tutorial walks through how these theoretical results manifest in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of Probability Sources Supported\n",
    "\n",
    "The LZ78 source is equivalent to the following formulation:\n",
    "1. Choose a prior distribution, $\\Pi$, on the simplex of probability mass functions over your alphabet.\n",
    "2. For every new node of the LZ78 tree, draw $\\Theta \\sim \\Pi$.\n",
    "3. At the current node of the LZ78 tree, draw the next symbol according to the corresponding $\\Theta$ value.\n",
    "\n",
    "So, an LZ78 source is characterized by the prior distribution, $\\Pi$.\n",
    "\n",
    "### 1.1 Supported Priors\n",
    "There are three types of prior distributions supported:\n",
    "1. **Dirichlet**: a Dirichlet($\\gamma, \\dots, \\gamma$) prior, which corresponds to the prior used in the `LZ78SPA` class.\n",
    "2. **Discrete** (only for binary alphabets): $\\Pi$ is some probability mass function over discrete points on the $[0, 1]$ interval.\n",
    "\n",
    "    **Note:** the proofs of the theoretical results for the LZ78 source require $\\Pi$ to have support on the full simplex, which does not hold for this distribution.\n",
    "\n",
    "3. **Dirac-Dirichlet Mixture** (only for binary alphabets): this is a perturbation of the above discrete distribution such that $\\Pi$ has support on the full $[0, 1]$ interval.\n",
    "    \n",
    "    This prior places weight `dirichlet_weight` on a Dirichlet($\\gamma, \\dots, \\gamma$) distribution and weight `1-dirichlet_weight` on a distribution with equal-height point masses at `dirac_loc` and `1-dirac_loc`.\n",
    "\n",
    "### 1.2 Dirichlet Prior Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.5\n",
    "lz78_source = DirichletLZ78Source(\n",
    "    alphabet_size=2, gamma=GAMMA, seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entropy Rate**\n",
    "The entropy rate for a Dirichlet LZ78 source over a binary alphabet is available in closed form (courtesy of Mathematica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQRT_PI = np.sqrt(np.pi)\n",
    "LOG2 = np.log(2)\n",
    "EULER_MASCH = 0.57721566490153286060651209008240243104215933593992\n",
    "def harmonic_number(n):\n",
    "    return sp.digamma(n + 1) + EULER_MASCH\n",
    "\n",
    "def binary_entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "def compute_lz78_dirichlet_entropy_rate(a):\n",
    "    if a == 0:\n",
    "        return 0\n",
    "    return -2 * GammaFunc(2*a) * (\n",
    "        4**(-a) * SQRT_PI * GammaFunc(a) * (\n",
    "            harmonic_number(a) - harmonic_number(2*a)\n",
    "        )\n",
    "    ) / (\n",
    "        GammaFunc(a + 1/2) * GammaFunc(a)**2 * LOG2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_rate = compute_lz78_dirichlet_entropy_rate(GAMMA)\n",
    "entropy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generating symbols and recording the scaled log probability at intervals**:\n",
    "\n",
    "The `generate_symbols` instance method returns the total log probability of the symbols generated in that function call.\n",
    "\n",
    "The `get_scaled_log_loss` method returns the scaled log probability of all symbols generated thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "lz78_source = DirichletLZ78Source(alphabet_size=2, gamma=GAMMA, seed=123)\n",
    "ns = [int(round(x)) for x in np.logspace(1, 7, 20)]\n",
    "\n",
    "prev_n = 0\n",
    "losses = []\n",
    "for n in ns:\n",
    "    lz78_source.generate_symbols(n - prev_n)\n",
    "    prev_n = n\n",
    "    losses.append(lz78_source.get_scaled_log_loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(\n",
    "    ns, np.ones(len(ns)) * entropy_rate,\n",
    "    'k--', linewidth=4, label=\"Entropy Rate\"\n",
    ")\n",
    "plt.plot(\n",
    "    ns, losses, \"-o\", linewidth=2, markersize=4,\n",
    "    color=\"red\", label=\"Log Probabilities\"\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.title(f\"Log Probability of Sequences from LZ78 Source\\n(Dirichlet Prior with $\\gamma=${GAMMA})\", fontdict={\"size\": 18})\n",
    "plt.xlabel(\"Number of Symbols\", fontdict={\"size\": 15})\n",
    "plt.ylabel(\"Log Probability\", fontdict={\"size\": 15})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Discrete Prior Example\n",
    "\n",
    "Let's consider the following prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_VALUES = [0.1, 0.5, 0.8]\n",
    "PROBABILITIES = [0.3, 0.5, 0.2]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(\n",
    "    THETA_VALUES, PROBABILITIES, \"r\"\n",
    ")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.title(f\"Prior Distribution\", fontdict={\"size\": 16})\n",
    "plt.xlabel(\"Theta\", fontdict={\"size\": 12})\n",
    "plt.ylabel(\"Probability\", fontdict={\"size\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generating symbols and recording the scaled log probability at intervals**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "lz78_source = DiscreteThetaLZ78Source(THETA_VALUES, PROBABILITIES)\n",
    "ns = [int(round(x)) for x in np.logspace(1, 7, 20)]\n",
    "\n",
    "prev_n = 0\n",
    "losses = []\n",
    "for n in ns:\n",
    "    lz78_source.generate_symbols(n - prev_n)\n",
    "    prev_n = n\n",
    "    losses.append(lz78_source.get_scaled_log_loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_rate = 0\n",
    "for (theta, prob) in zip(THETA_VALUES, PROBABILITIES):\n",
    "    entropy_rate += prob * binary_entropy(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(\n",
    "    ns, np.ones(len(ns)) * entropy_rate,\n",
    "    'k--', linewidth=4, label=\"Entropy Rate\"\n",
    ")\n",
    "plt.plot(\n",
    "    ns, losses, \"-o\", linewidth=2, markersize=4,\n",
    "    color=\"red\", label=\"Log Probabilities\"\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.title(f\"Log Probability of Sequences from LZ78 Source\\n(Point Mass Prior)\", fontdict={\"size\": 18})\n",
    "plt.xlabel(\"Number of Symbols\", fontdict={\"size\": 15})\n",
    "plt.ylabel(\"Log Probability\", fontdict={\"size\": 15})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dirac-Dirichlet Mixture Example\n",
    "\n",
    "Now, conider a prior distribution with weight 0.1 on a uniform distribution and weight 0.9 on the distribution with point masses at 0.2 and 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "lz78_source = DiracDirichletLZ78Source(gamma=1, dirichlet_weight=0.1, dirac_loc=0.2)\n",
    "ns = [int(round(x)) for x in np.logspace(1, 7, 20)]\n",
    "\n",
    "prev_n = 0\n",
    "losses = []\n",
    "for n in ns:\n",
    "    lz78_source.generate_symbols(n - prev_n)\n",
    "    prev_n = n\n",
    "    losses.append(lz78_source.get_scaled_log_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lz78_dirac_dirichlet_entropy_rate(gamma, dirichlet_weight, dirac_loc):\n",
    "    dirac_entropy = binary_entropy(dirac_loc)\n",
    "    dirichlet_entropy = compute_lz78_dirichlet_entropy_rate(gamma)\n",
    "    return (1-dirichlet_weight) * dirac_entropy + dirichlet_weight * dirichlet_entropy\n",
    "entropy_rate = compute_lz78_dirac_dirichlet_entropy_rate(1, 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(\n",
    "    ns, np.ones(len(ns)) * entropy_rate,\n",
    "    'k--', linewidth=4, label=\"Entropy Rate\"\n",
    ")\n",
    "plt.plot(\n",
    "    ns, losses, \"-o\", linewidth=2, markersize=4,\n",
    "    color=\"red\", label=\"Log Probabilities\"\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.title(f\"Log Probability of Sequences from LZ78 Source\\n(Dirac-DirichletPrior)\", fontdict={\"size\": 18})\n",
    "plt.xlabel(\"Number of Symbols\", fontdict={\"size\": 15})\n",
    "plt.ylabel(\"Log Probability\", fontdict={\"size\": 15})\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
