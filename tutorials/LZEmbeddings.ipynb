{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: LZ Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Instructions\n",
    "\n",
    "### A. Rust LZ78 Library\n",
    "You need to install Rust and Maturin, and then install the Python bindings for the `lz78` library as an editable Python package.\n",
    "1. Install Rust: [Instructions](https://www.rust-lang.org/tools/install).\n",
    "    - After installing Rust, close and reopen your terminal before proceeding.\n",
    "2. If applicable, switch to the desired Python environment.\n",
    "3. Install Maturin: `pip install maturin`\n",
    "4. Install the `lz78` Python package: `cd crates/python && maturin develop -r && cd ../..`\n",
    "\n",
    "**NOTE**: If you use virtual environments, you may run into an issue. If you are a conda user, it's possible the `(base)` environment may be activated on startup. `maturin` does not allow for two active virtual environments (ie. via `venv` and `conda`). You must make sure only one is active. One solution is to run `conda deactivate` in preference of your `venv` based virtual environment.\n",
    "\n",
    "**NOTE**: If you are using MacOS, you may run into the following error with `maturin develop`:\n",
    "```\n",
    "error [E0463]: can't find crate for core\n",
    "    = note: the X86_64-apple-darwin target may not be installed\n",
    "    = help: consider downloading the target with 'rustup target add x86_64-apple-darwin'\n",
    "```\n",
    "Running the recommended command `rustup target add x86_64-apple-darwin` should resolve the issue.\n",
    "\n",
    "### B. LZ78 Embeddings\n",
    "From the root directory of the repository, run\n",
    "```\n",
    "pip install --editable .\n",
    "```\n",
    "\n",
    "### **Warning**\n",
    "Sometimes, Jupyter doesn't register that a cell containing code from the `lz78` library has started running, so it seems like the cell is waiting to run until it finishes.\n",
    "This can be annoying for operations that take a while to run, and **can be remedied by putting `stdout.flush()` at the beginning of the cell**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from tqdm import tqdm\n",
    "from lz_embed.classical import BasicLZSpectrum, AlphabetInfo, BasicNGramSpectrum\n",
    "from lz_embed.transformer_based import LZPlusEmbeddingModel, WeightType, EmbeddingType\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "This notebook contains two simple ways of utilizing LZ78 to create an embedding.\n",
    "A high-level description of each method is below; see the rest of the notebook for more details: \n",
    "1. **The LZ Spectrum**: computes the proportion of every symbol in the alphabet for each node of the LZ78 tree (in a pre-defined order) and concatenates all of them. This is simple, but only works well for small alphabets, or else the length of the resulting embedding grows too large.\n",
    "2. **Combining LZ with a pretrained embedding model**: first, some training data is used to build an LZ78 tree. Then, the embeddings for all nodes can be computed using a pretrained embedding model.\n",
    "\n",
    "    To embed a text, we first use the LZ78 tree to parse the text into phrases. Then, each phrase is embedded based on the embedding of the corresponding leaf, and the weighted average of phrase embeddings is taken.\n",
    "\n",
    "    _Note_: for this notebook, the embeddings of all nodes are not actually computed _a priori_, but rather they are computed at evaluation time. This is just to facilitate quick experiments.\n",
    "\n",
    "#### **Disclaimer: this is preliminary work, and not very polished, documented, or debugged.**\n",
    "\n",
    "## 4. LZ Spectrum\n",
    "### Methodology\n",
    "The LZ78 embeds a sequence over an alphabet, $\\mathcal{A}$ of size $A$ as follows:\n",
    "1. It builds an LZ78 tree using the sequence (for now, with no training required). There is an option to limit the depth of the tree, which is recommended.\n",
    "2. The nodes of the tree are ordered based on their corresponding phrases, as follows:\n",
    "    - Phrase $x \\in \\mathcal{A}^n$ is less than phrase $b \\in \\mathcal{A}^m$ iff $n < m$ or $n = m$ and $\\sum_{i=1}^n x_i A^{i-1} < \\sum_{i=1}^n y_i A^{i-1}$\n",
    "\n",
    "3. The empirical distribution of symbols seen while at each node (with the last component cut out, as it is a deterministic function of the first $A-1$ components) are concatenated together, in the ordering defined above. If there are \"gaps\" (nodes missing in a level of the tree), they are filled in with the uniform distribution.\n",
    "\n",
    "### Instantiation\n",
    "To instantiate a `BasicLZSpectrum`, you need to specify information about the alphabet of the sequences you are encoding.\n",
    "This is via the `AlphabetInfo` dataclass, where you specify either:\n",
    "1. `AlphabetInfo(alphabet_size=A)`, for encoding integer sequences, or\n",
    "2. `AlphabetInfo(valid_character_string=\"abcdefgh...\")` for encoding strings. This specifies that strings being encoded can only consist of characters in `valid_character_string`.\n",
    "\n",
    "You can also specify `max_depth` (which is recommended), which stops adding new leaves to the LZ tree below the specified depth.\n",
    "\n",
    "**Fixed-length embeddings**: If the `max_depth` argument is specified, then the tensors returned by the `BasicLZSpectrum.embed` function are guaranteed to be a fixed length.\n",
    "You can also specify the `fixed_length` argument upon instantiation to either pad (with the value $1/A$) or truncate embeddings to a specified length.\n",
    "\n",
    "**N-Gram Spectrum**: Alternatively, you can use a `BasicNGramSpectrum`, which is instantiated by passing in an alphabet size and context length (`n`).\n",
    "An `n`-gram spectrum should converge with less data, at a very slight computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLZSpectrum(AlphabetInfo(alphabet_size=3), max_depth=4)\n",
    "# model = BasicNGramSpectrum(alpha_size=3, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "You can embed a sequence by calling `encode_single`, or a list of sequences by calling `encode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "seq1 = [0, 1, 2, 1, 2, 0, 1, 1, 1, 2, 2, 2] * 5000\n",
    "seq2 = np.random.randint(0, 3, size=(50000))\n",
    "emb = model.encode_single(seq1)\n",
    "(emb1, emb2) = model.encode([seq1, seq2])\n",
    "assert np.allclose(emb1, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.stem(emb1, \"red\")\n",
    "plt.grid(True)\n",
    "plt.title(\"LZ Spectrum of Structured Sequence\", fontdict={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.stem(emb2, \"blue\")\n",
    "plt.grid(True)\n",
    "plt.title(\"LZ Spectrum of Random Sequence\", fontdict={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues and Next Steps\n",
    "One key issue with this type of embedding is that the length of the embedding vector scales as $(A-1)A^d$, where $A$ is the alphabet size and $d$ is the depth of the tree. This will result in unreasonable-large embedding vectors for domains such as text.\n",
    "\n",
    "Also, short sequences produce smaller LZ trees, resulting in an embedding vector that is mostly $1/A$-valued, which may be undesirable for downstream tasks.\n",
    "\n",
    "#### **Some next steps:**\n",
    "- Use an n-gram model for this instead of LZ\n",
    "- Select nodes of the LZ tree to include in the embedding (sparsify the tree) based on some training data\n",
    "- Think of other forms of dimensionality reduction for the embedding (maybe PCA over a training set)\n",
    "\n",
    "#### **Note**: this may be more useful for a small-alphabet use case like DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LZ + Embedding Model\n",
    "\n",
    "### Methodology\n",
    "1. **Train**: using training data, build an LZ78 tree.\n",
    "2. For each node in the tree, compute the embedding of the corresponding phrase using some embedding model. For now, this does **not** happen _a priori_ and only happens as phrases are encountered when embedding a sequence.\n",
    "3. **Embedding**: To embed a sequence, first use the LZ78 SPA tree to compute:\n",
    "    - The embeddings of phrases that the sequence parses into, and\n",
    "    - The average log loss of each such phrase.\n",
    "4. The final embedding is the weighted average of the embeddings, with the weights proportional to the average of $2^{\\text{symbol log loss}}$ (from [this blog post](https://huggingface.co/blog/Pringled/model2vec), more probable contexts should be weighted less because they do not help distinguish between different texts as well).\n",
    "\n",
    "### Instantiation\n",
    "To instantiate an `LZPlusEmbeddingModel` object, you need to pass in the Huggingface model string of the base embedding model.\n",
    "\n",
    "For now, you also need to pass in a string of valid characters, like for `AlphabetInfo` in **Section 4**.\n",
    "This is subject to change as the model becomes more sophisticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using OpenAI Embeddings, you need to record your\n",
    "# API key in the environment variables\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: running a model locally via `transformers`\n",
    "# model = LZPlusEmbeddingModel(\n",
    "#     inner_model_name=\"Alibaba-NLP/gte-Qwen2-7B-instruct\", \n",
    "#     inner_model_type=EmbeddingType.TRANSFORMERS,\n",
    "#     device=\"cuda:6\",\n",
    "#     valid_character_string=\"abcdefghijklmnopqrstuvwxyz \",\n",
    "#     make_lowercase=True,\n",
    "#     weight_type=WeightType.UNIFORM\n",
    "# )\n",
    "\n",
    "# Option 2: using an OpenAI Embedding model\n",
    "model = LZPlusEmbeddingModel(\n",
    "    inner_model_name=\"text-embedding-3-large\",\n",
    "    inner_model_type=EmbeddingType.OPENAI,\n",
    "    valid_character_string=\"abcdefghijklmnopqrstuvwxyz \",\n",
    "    make_lowercase=True,\n",
    "    weight_type=WeightType.UNIFORM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training is done via the `LZPlusEmbeddingModel.train_spa` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"salesforce/Wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "stdout.flush()\n",
    "for _ in tqdm(range(EPOCHS)):\n",
    "    for text in (dataset[\"train\"]):\n",
    "        text = text[\"text\"]\n",
    "        if not text:\n",
    "            continue\n",
    "        model.train_spa(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout.flush()\n",
    "model.spa.prune(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The LZ tree has {model.spa.get_total_nodes() / 1e6} million nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Embedding is done via the same iterface as in **Section 4**.\n",
    "Embeddings returned are the same dimensionality as the base embedding model (for now).\n",
    "\n",
    "Below is a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'how much protein should a female eat',\n",
    "    'summit define'\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "    \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "]\n",
    "input_texts = queries + documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spa.set_inference_config(\n",
    "    ensemble_type=\"entropy\", ensemble_n=3, backshift_parsing=True, backshift_ctx_len=3\n",
    ")\n",
    "model.weight_type = WeightType.UNIFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, you can turn debugging on to get the phrases being embedded\n",
    "# and their weights\n",
    "\n",
    "model.debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encode(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = (emb[:2] @ emb[2:].T) * 100\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"Correlation of Query {i} with...\", end=\"\\t\")\n",
    "    for j in range(2):\n",
    "        print(f\"Document {j}: {round(scores[i, j].item(), 2)}\", end=\"\")\n",
    "        if j == 0:\n",
    "            print(\",\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues and Next Steps\n",
    "The number of nodes in an LZ tree is very large, which means the \"model size\" can be quite large (and the plot for the competition is \"model size vs. accuracy\").\n",
    "\n",
    "Also, it is unclear how much this differs to existing works that average the embeddings of tokens produced via, e.g., byte-pair encoding.\n",
    "\n",
    "#### **Some next steps**:\n",
    "1. Usng the embeddings for context aggregation, which will reduce the model size\n",
    "2. Try this out on some downstream tasks\n",
    "3. Find a good training set for growing the LZ tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
